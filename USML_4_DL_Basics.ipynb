{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liang-faan/python-ml/blob/main/USML_4_DL_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1spxhS0t2Y4",
        "outputId": "f99d2b4b-0007-46b3-b625-85ddd1bab0d3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun 11 07:18:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    35W / 250W |    929MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odhi4kMFnKlG"
      },
      "source": [
        "### Basics of PyTorch modeling\n",
        "Here, I will outline the major components of a Deep Learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4lldfqLcmmN"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpEL5jrUtBD5"
      },
      "source": [
        "Step 0: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TNDlggH4d4M"
      },
      "source": [
        "# Let's download the data first\n",
        "dataset = datasets.MNIST('data', download=True, train=True, transform=transforms.ToTensor())\n",
        "\n",
        "# The dataset is split into X (image) and y (label)\n",
        "# Here we check the first image\n",
        "# (which is an image of 28 by 28 pixels with the label 5)\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dASKzeyA4qX2"
      },
      "source": [
        "# Separate the image from the label\n",
        "img_batch, label = dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIlPi05z6NOz",
        "outputId": "5f3f18b3-5457-4559-cd3b-7eb3748e1b8f"
      },
      "source": [
        "# .shape is an important method that we use to inspect the\n",
        "# dimensions of a data sample\n",
        "\n",
        "# Here, we have 1 sample with 28 x 28 pixels\n",
        "# The first number indicates batch size.\n",
        "# We usually process a number of samples at a time, so if we\n",
        "# take 32 samples per batch, then each img_batch will have the\n",
        "# shape (32, 28, 28)\n",
        "img_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "hpQKBklL4zw7",
        "outputId": "75ed4530-169e-490d-f84c-20e419c3c618"
      },
      "source": [
        "# Let's print the first image to see what it looks like.\n",
        "# The first image is indexed 0 in the batch.\n",
        "\n",
        "plt.imshow(img_batch[0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa480c756d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1XoRcGqtDDz"
      },
      "source": [
        "# Now let's split the entire dataset into train and validation set\n",
        "train, val = random_split(dataset, [55000, 5000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M16Pb1xV8G2r"
      },
      "source": [
        "# Loading data in PyTorch and Tensorflow is done via special classes\n",
        "# DataLoaders makes it easier to work with data across the pipeline\n",
        "train_loader = DataLoader(train, batch_size=32)\n",
        "val_loader = DataLoader(val, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmNWaZ1MnaAf"
      },
      "source": [
        "### Step 1: Define a model architecture\n",
        "Using sequential means a fully connected neural network where all sucessive nodes are connected to the nodes in the previous layer.\n",
        "\n",
        "Let's try using it to predict images with dimension 28 by 28 using a multi-layer perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEqBD73MndyS"
      },
      "source": [
        "# Note that the tensor separates the image into a matrix that is 28 by 28\n",
        "# For images, each pixel is a feature, therefore, we can represent each image\n",
        "# with 28 x 28 = 784 features.\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(28 * 28, 64),  # (input dimension, output dimension) << input layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),  # (input dimension, output dimension)  << hidden layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)  # (input dimension, output dimension) << output layer (10 classes)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUOm5-8RqWR7"
      },
      "source": [
        "What are activation functions? https://mlfromscratch.com/activation-functions-explained/#/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAQSJHcap0kw"
      },
      "source": [
        "### Step 2: Define an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm5oOL-0o2c1"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnKAkd6EqaHO"
      },
      "source": [
        "What are optimizers? https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2SaGsFYqfha"
      },
      "source": [
        "### Step 3: Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GctW28uIoHQI"
      },
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVATqEHtqngW"
      },
      "source": [
        "### Step 4: Define training loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bwyQ-t6qrSM",
        "outputId": "6e55a2aa-2428-4674-d417-27b461a70d35"
      },
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    train_losses = list()\n",
        "    train_accuracy = list()\n",
        "    for batch in train_loader:\n",
        "        x, y = batch  # In a dataloader, x is the data and y is the label\n",
        "\n",
        "        # From a dataloader, x is of the shape (batch, channels, dimensions)\n",
        "        # Example (32, 1, 784): Recall that 784 is 28 x 28 which is the image.\n",
        "        # Since we only have 1 channel, we can remove it to make it (32, 784)\n",
        "        b = x.size(0)\n",
        "        x = x.view(b, -1)\n",
        "\n",
        "        # Step 1: Forward the input feature values through the network\n",
        "        logits = model(x)\n",
        "\n",
        "        # Step 2: Compute the objective function\n",
        "        J = loss(logits, y)\n",
        "\n",
        "        # Step 3: Clean any previous accumulated gradients by setting it to 0\n",
        "        model.zero_grad()  # This is like setting params.grad to zero\n",
        "\n",
        "        # Step 4: Accumulate the partial derivatives of J wrt parameters\n",
        "        J.backward()  # This is like summing params.grad with (dJ/dparams)\n",
        "\n",
        "        # Step 5: Apply a step in the opposite direction of the gradient\n",
        "        optimizer.step()  # This is like params = params - learning_rate * params.grad\n",
        "\n",
        "        train_losses.append(J.item())\n",
        "        train_accuracy.append(y.eq(logits.argmax(dim=1)).float().mean())\n",
        "\n",
        "    val_losses = list()\n",
        "    val_accuracy = list()\n",
        "    for batch in val_loader:\n",
        "        x, y = batch  # In a dataloader, x is the data and y is the label\n",
        "\n",
        "        # From a dataloader, x is of the shape (batch, channels, dimensions)\n",
        "        b = x.size(0)\n",
        "        x = x.view(b, -1)\n",
        "\n",
        "        # Forward the input feature values through the network\n",
        "        # ## Since validation don't update weights, we don't need to load the\n",
        "        # ## gradients\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "\n",
        "        # Compute the objective function\n",
        "        J = loss(logits, y)\n",
        "\n",
        "        val_losses.append(J.item())\n",
        "        val_accuracy.append(y.eq(logits.argmax(dim=1)).float().mean())\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, train loss: {torch.tensor(train_losses).mean():.2f}, val loss: {torch.tensor(val_losses).mean():.2f}')\n",
        "    print(f'        train acc: {torch.tensor(train_accuracy).mean():.2f}, val acc: {torch.tensor(val_accuracy).mean():.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train loss: 1.23, val loss: 0.48\n",
            "        train acc: 0.64, val acc: 0.86\n",
            "Epoch 2, train loss: 0.41, val loss: 0.34\n",
            "        train acc: 0.89, val acc: 0.90\n",
            "Epoch 3, train loss: 0.33, val loss: 0.29\n",
            "        train acc: 0.91, val acc: 0.91\n",
            "Epoch 4, train loss: 0.28, val loss: 0.26\n",
            "        train acc: 0.92, val acc: 0.92\n",
            "Epoch 5, train loss: 0.25, val loss: 0.23\n",
            "        train acc: 0.93, val acc: 0.93\n",
            "Epoch 6, train loss: 0.23, val loss: 0.21\n",
            "        train acc: 0.93, val acc: 0.94\n",
            "Epoch 7, train loss: 0.21, val loss: 0.20\n",
            "        train acc: 0.94, val acc: 0.94\n",
            "Epoch 8, train loss: 0.19, val loss: 0.19\n",
            "        train acc: 0.95, val acc: 0.94\n",
            "Epoch 9, train loss: 0.18, val loss: 0.18\n",
            "        train acc: 0.95, val acc: 0.95\n",
            "Epoch 10, train loss: 0.17, val loss: 0.17\n",
            "        train acc: 0.95, val acc: 0.95\n",
            "Epoch 11, train loss: 0.16, val loss: 0.16\n",
            "        train acc: 0.95, val acc: 0.95\n",
            "Epoch 12, train loss: 0.15, val loss: 0.15\n",
            "        train acc: 0.96, val acc: 0.95\n",
            "Epoch 13, train loss: 0.14, val loss: 0.15\n",
            "        train acc: 0.96, val acc: 0.96\n",
            "Epoch 14, train loss: 0.13, val loss: 0.14\n",
            "        train acc: 0.96, val acc: 0.96\n",
            "Epoch 15, train loss: 0.13, val loss: 0.14\n",
            "        train acc: 0.96, val acc: 0.96\n",
            "Epoch 16, train loss: 0.12, val loss: 0.13\n",
            "        train acc: 0.97, val acc: 0.96\n",
            "Epoch 17, train loss: 0.11, val loss: 0.13\n",
            "        train acc: 0.97, val acc: 0.96\n",
            "Epoch 18, train loss: 0.11, val loss: 0.12\n",
            "        train acc: 0.97, val acc: 0.96\n",
            "Epoch 19, train loss: 0.10, val loss: 0.12\n",
            "        train acc: 0.97, val acc: 0.96\n",
            "Epoch 20, train loss: 0.10, val loss: 0.12\n",
            "        train acc: 0.97, val acc: 0.97\n",
            "Epoch 21, train loss: 0.10, val loss: 0.11\n",
            "        train acc: 0.97, val acc: 0.97\n",
            "Epoch 22, train loss: 0.09, val loss: 0.11\n",
            "        train acc: 0.97, val acc: 0.97\n",
            "Epoch 23, train loss: 0.09, val loss: 0.11\n",
            "        train acc: 0.97, val acc: 0.97\n",
            "Epoch 24, train loss: 0.08, val loss: 0.11\n",
            "        train acc: 0.98, val acc: 0.97\n",
            "Epoch 25, train loss: 0.08, val loss: 0.10\n",
            "        train acc: 0.98, val acc: 0.97\n",
            "Epoch 26, train loss: 0.08, val loss: 0.10\n",
            "        train acc: 0.98, val acc: 0.97\n",
            "Epoch 27, train loss: 0.07, val loss: 0.10\n",
            "        train acc: 0.98, val acc: 0.97\n",
            "Epoch 28, train loss: 0.07, val loss: 0.10\n",
            "        train acc: 0.98, val acc: 0.97\n",
            "Epoch 29, train loss: 0.07, val loss: 0.10\n",
            "        train acc: 0.98, val acc: 0.97\n",
            "Epoch 30, train loss: 0.07, val loss: 0.10\n",
            "        train acc: 0.98, val acc: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EApwRHJ-wbmK"
      },
      "source": [
        "As the number of epochs grow, the training and validation accuracy will increase until a point where the validation accuracy will start to fall. This is the point where overfitting happens.\n",
        "\n",
        "If overfitting occurs after an acceptable validation accuracy, then we can simply stop the training at the epoch number right before overfitting happens.\n",
        "\n",
        "What if we have not achieved the acceptable validation accuracy? In this case we need to perform some overfitting techniques.\n",
        "> The most common one is the dropout, which randomly sets some node values to zero.\n",
        ">\n",
        "> This is very similar to bagging ensembles, where each model sees only a subset of values and features.\n",
        ">\n",
        "> In deep learning, each epoch is a small model. Successive epochs adjusts the weights of the previous to get better accuracy. Dropout adds the bagging mechanism to this.\n",
        "\n",
        "In the next section, we will implement the dropout. But let's class the model to make it easier to maintain.\n",
        "> This class is also the default way to construct PyTorch models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz0gi0_OYNsy"
      },
      "source": [
        "Because we are switching models, please remember to restart your colab instance!\n",
        "> Go to the menu bar and click Runtime > Restart Runtime.\n",
        ">\n",
        "> Then load all the imports, data, and dataloader codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsNGE6GBwbVH"
      },
      "source": [
        "class ImageNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(28 * 28, 64)\n",
        "        self.l2 = nn.Linear(64, 64)\n",
        "        self.l3 = nn.Linear(64, 10)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = nn.functional.relu(self.l1(x))\n",
        "        h2 = nn.functional.relu(self.l2(h1))\n",
        "        dropout = self.dropout(h2)\n",
        "        # dropout = self.dropout(h2 + h1)  # This is known as a residual connection\n",
        "        logits = self.l3(dropout)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNArS92lXden"
      },
      "source": [
        "We are also going to utilize GPU. Always remember:\n",
        "1. The model and data must both reside in the same computational device, i.e. both on CPU or both on GPU.\n",
        "> Data refers to both the X and the y.\n",
        "2. To switch between CPU and GPU, we use the .to() method.\n",
        "> model.to('cuda') or model.to('cpu')\n",
        ">\n",
        "> x.to('cuda') or x.to('cpu')\n",
        ">\n",
        "> y.to('cuda') or y.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJno5ifAxANU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14568ce1-0a07-4307-8e77-52ede561d3f7"
      },
      "source": [
        "# Call the model and load it into the GPU\n",
        "model = ImageNN()\n",
        "model.to('cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageNN(\n",
              "  (l1): Linear(in_features=784, out_features=64, bias=True)\n",
              "  (l2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (l3): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-lZ1DCsxH0f"
      },
      "source": [
        "# Define optimizer\n",
        "# Here we use a more efficient optimizer called the Adam, see how fast it\n",
        "# reaches the minimum loss.\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "# Define loss\n",
        "loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQLLwu87xWwV",
        "outputId": "0e6390b8-8eda-481a-9ab8-8e48ec248692"
      },
      "source": [
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_losses = list()\n",
        "    train_accuracy = list()\n",
        "    model.train()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        x, y = batch  # In a dataloader, x is the data and y is the label\n",
        "\n",
        "        # From a dataloader, x is of the shape (batch, channels, dimensions)\n",
        "        # Example (32, 1, 784): Recall that 784 is 28 x 28 which is the image.\n",
        "        # Since we only have 1 channel, we can remove it to make it (32, 784)\n",
        "        b = x.size(0)\n",
        "        x = x.view(b, -1).to('cuda')\n",
        "        y = y.to('cuda')\n",
        "\n",
        "        # Step 1: Forward the input feature values through the network\n",
        "        logits = model(x)\n",
        "\n",
        "        # Step 2: Compute the objective function\n",
        "        J = loss(logits, y)\n",
        "\n",
        "        # Step 3: Clean any previous accumulated gradients by setting it to 0\n",
        "        model.zero_grad()  # This is like setting params.grad to zero\n",
        "\n",
        "        # Step 4: Accumulate the partial derivatives of J wrt parameters\n",
        "        J.backward()  # This is like summing params.grad with (dJ/dparams)\n",
        "\n",
        "        # Step 5: Apply a step in the opposite direction of the gradient\n",
        "        optimizer.step()  # This is like params = params - learning_rate * params.grad\n",
        "\n",
        "        train_losses.append(J.item())\n",
        "        train_accuracy.append(y.eq(logits.argmax(dim=1)).float().mean())\n",
        "\n",
        "    val_losses = list()\n",
        "    val_accuracy = list()\n",
        "    model.eval()\n",
        "\n",
        "    for batch in val_loader:\n",
        "        x, y = batch  # In a dataloader, x is the data and y is the label\n",
        "\n",
        "        # From a dataloader, x is of the shape (batch, channels, dimensions)\n",
        "        b = x.size(0)\n",
        "        x = x.view(b, -1).to('cuda')\n",
        "        y = y.to('cuda')\n",
        "\n",
        "        # Forward the input feature values through the network\n",
        "        # ## Since validation don't update weights, we don't need to load the\n",
        "        # ## gradients\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "\n",
        "        # Compute the objective function\n",
        "        J = loss(logits, y)\n",
        "\n",
        "        val_losses.append(J.item())\n",
        "        val_accuracy.append(y.eq(logits.argmax(dim=1)).float().mean())\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, train loss: {torch.tensor(train_losses).mean():.2f}, val loss: {torch.tensor(val_losses).mean():.2f}')\n",
        "    print(f'         train acc: {torch.tensor(train_accuracy).mean():.3f}, val acc: {torch.tensor(val_accuracy).mean():.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, train loss: 0.28, val loss: 0.23\n",
            "         train acc: 0.918, val acc: 0.935\n",
            "Epoch 2, train loss: 0.19, val loss: 0.21\n",
            "         train acc: 0.948, val acc: 0.948\n",
            "Epoch 3, train loss: 0.16, val loss: 0.21\n",
            "         train acc: 0.956, val acc: 0.950\n",
            "Epoch 4, train loss: 0.15, val loss: 0.18\n",
            "         train acc: 0.960, val acc: 0.957\n",
            "Epoch 5, train loss: 0.14, val loss: 0.19\n",
            "         train acc: 0.963, val acc: 0.953\n",
            "Epoch 6, train loss: 0.13, val loss: 0.20\n",
            "         train acc: 0.966, val acc: 0.961\n",
            "Epoch 7, train loss: 0.12, val loss: 0.22\n",
            "         train acc: 0.969, val acc: 0.956\n",
            "Epoch 8, train loss: 0.12, val loss: 0.19\n",
            "         train acc: 0.969, val acc: 0.960\n",
            "Epoch 9, train loss: 0.12, val loss: 0.20\n",
            "         train acc: 0.971, val acc: 0.958\n",
            "Epoch 10, train loss: 0.11, val loss: 0.19\n",
            "         train acc: 0.972, val acc: 0.967\n",
            "Epoch 11, train loss: 0.11, val loss: 0.21\n",
            "         train acc: 0.973, val acc: 0.965\n",
            "Epoch 12, train loss: 0.10, val loss: 0.17\n",
            "         train acc: 0.975, val acc: 0.967\n",
            "Epoch 13, train loss: 0.10, val loss: 0.19\n",
            "         train acc: 0.976, val acc: 0.962\n",
            "Epoch 14, train loss: 0.10, val loss: 0.29\n",
            "         train acc: 0.977, val acc: 0.962\n",
            "Epoch 15, train loss: 0.10, val loss: 0.20\n",
            "         train acc: 0.976, val acc: 0.958\n",
            "Epoch 16, train loss: 0.10, val loss: 0.24\n",
            "         train acc: 0.977, val acc: 0.965\n",
            "Epoch 17, train loss: 0.10, val loss: 0.27\n",
            "         train acc: 0.977, val acc: 0.960\n",
            "Epoch 18, train loss: 0.10, val loss: 0.24\n",
            "         train acc: 0.976, val acc: 0.962\n",
            "Epoch 19, train loss: 0.10, val loss: 0.25\n",
            "         train acc: 0.977, val acc: 0.962\n",
            "Epoch 20, train loss: 0.10, val loss: 0.28\n",
            "         train acc: 0.977, val acc: 0.959\n",
            "Epoch 21, train loss: 0.09, val loss: 0.29\n",
            "         train acc: 0.980, val acc: 0.962\n",
            "Epoch 22, train loss: 0.10, val loss: 0.31\n",
            "         train acc: 0.977, val acc: 0.957\n",
            "Epoch 23, train loss: 0.09, val loss: 0.26\n",
            "         train acc: 0.978, val acc: 0.962\n",
            "Epoch 24, train loss: 0.09, val loss: 0.37\n",
            "         train acc: 0.979, val acc: 0.958\n",
            "Epoch 25, train loss: 0.09, val loss: 0.31\n",
            "         train acc: 0.978, val acc: 0.962\n",
            "Epoch 26, train loss: 0.09, val loss: 0.29\n",
            "         train acc: 0.978, val acc: 0.962\n",
            "Epoch 27, train loss: 0.09, val loss: 0.31\n",
            "         train acc: 0.980, val acc: 0.963\n",
            "Epoch 28, train loss: 0.10, val loss: 0.29\n",
            "         train acc: 0.977, val acc: 0.961\n",
            "Epoch 29, train loss: 0.09, val loss: 0.28\n",
            "         train acc: 0.979, val acc: 0.960\n",
            "Epoch 30, train loss: 0.09, val loss: 0.28\n",
            "         train acc: 0.979, val acc: 0.966\n"
          ]
        }
      ]
    }
  ]
}